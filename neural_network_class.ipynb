{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all import packages\n",
    "from scipy.stats import truncnorm\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def truncated_normal(mean = 0, sd = 1, low = 0, upp = 10):\n",
    "    return truncnorm((low - mean)/sd,\n",
    "                     (upp - mean)/sd,\n",
    "                     loc = mean,\n",
    "                     scale = sd)\n",
    "\n",
    "\n",
    "##### Layer Class #####\n",
    "\n",
    "class Activated_Linear_Layer:\n",
    "\n",
    "    #Initialization of the parameters of the Layer\n",
    "    def __init__(self, num_of_inputs, num_of_outputs, activation = 'relu', bias = True, lamda = 0, beta = 0.9):\n",
    "        self.bias = bias\n",
    "        self.lamda = lamda\n",
    "        self.activation = activation    # Either 'relu' or 'softmax'\n",
    "        rad = 1/(np.sqrt(num_of_inputs + self.bias))\n",
    "        truncate = truncated_normal(mean = 0, sd = 1, low = -rad, upp = rad)\n",
    "        #random initialization of weights based off of truncnorm (use seed 42 for testing purposes)\n",
    "        # and if we have biases, add one to the dimension to simulate bias node\n",
    "        self.weights = truncate.rvs((num_of_outputs, num_of_inputs + self.bias))                        #(W1) & (W2)\n",
    "        self.input_array_bnodes = []\n",
    "        self.beta = beta\n",
    "        self.velocity = np.zeros(self.weights.shape)\n",
    "    \n",
    "    #Forward propogation through the ReLU Layer\n",
    "    def forward_pass(self, input_array):                                                                \n",
    "        #if we are using biases then add a \"pseudo-node\" with values 1, to the input to work as the bias\n",
    "        if self.bias: \n",
    "            bias_nodes = np.tile(1,(input_array.shape[0],1))\n",
    "            self.input_array_bnodes= np.concatenate((input_array, bias_nodes), axis = 1)                #(X+) & (A1+)\n",
    "        else:\n",
    "            self.input_array_bnodes = input_array                                                       # or just (X) & (A1)\n",
    "        self.output_array= np.dot(self.input_array_bnodes, self.weights.T)                              #(Z1 = X.W1t) & (Z2 = A1.W2t)\n",
    "        #ReLU activation of the linear output\n",
    "        if self.activation == 'relu': \n",
    "            activated_output_array= np.maximum(self.output_array, 0.0)                                  #A1 = ReLU(Z1)\n",
    "        #Softmax activation of the linear output\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_array = np.exp(self.output_array)\n",
    "            activated_output_array = exp_array / np.sum(exp_array, axis = 1)[:,np.newaxis]              #A2 = Softmax(Z2)  corrected so that it does proper softmax when workling with mini batches                         \n",
    "        return activated_output_array\n",
    "\n",
    "    #Backward propogation through the ReLU Layer and weight updates\n",
    "    def backward_pass (self, delta_activated_output, learning_rate = 0.01):                                                   #(dA2) & (dA1)\n",
    "        m = delta_activated_output.shape[0]                                                             #m = batch size\n",
    "        \n",
    "        #Derivative of ReLU activation fuction\n",
    "        if self.activation == 'relu':\n",
    "            delta_output = np.multiply(delta_activated_output, (self.output_array>= 0))                 #(dZ1 = ReLU'(dA1))\n",
    "        #Derivative of the Softmax acivation function (This is not calculated from the cost function, but straight from dZ = y_hat - y)\n",
    "        elif self.activation == 'softmax':\n",
    "            delta_output = delta_activated_output                                                       #(dZ2 = dA2 = y_hat - y)\n",
    "        #Derivative of input to pass on to previous layers\n",
    "        delta_input_array = np.dot(delta_output, self.weights)                                          #(dA1 = dZ2.W2) & (dX = dZ1.W1)\n",
    "        if self.bias: delta_input_array = delta_input_array[:,:-1]                                      #if we have a bias node, the we have extra on dA\n",
    "        #Derivative of weights, in order to update, with L2 regularization and momentum\n",
    "        delta_weights = np.dot(delta_output.T,self.input_array_bnodes) + (self.lamda/m)*self.weights    #(dW1 = dZ1.Xt + (lamda/m)*W1) & (dW2 = dZ2.A1t + (lamda/m)*W2)\n",
    "        self.velocity = self.beta*self.velocity + (1-self.beta)*delta_weights                           #(V[t] = beta*V[t-1] + (1-beta)*dW)\n",
    "        self.weights -= learning_rate*self.velocity                                                     #(W1 = W1 - a*V) & (W2 = W2 - a*dV) #check to see if V is common over each layer, or seperate.. here it is applied diff per layer\n",
    "\n",
    "        return delta_input_array\n",
    "\n",
    "\n",
    "\n",
    "##### Neural Network Class #####\n",
    "\n",
    "class CK_NN :\n",
    "\n",
    "    #Initialization of the Neural Network with 3 layer (Input, Hidden-ReLU, Output-Softmax)\n",
    "    def __init__ (self, layer_sizes = [3072,128,10], layer_activations = ['relu', 'softmax'], dropout_prob = 0.2, bias = True, lamda = 0, beta = 0.9):\n",
    "\n",
    "        #we assume that len(layer_sizes) = len(layer_activations) + 1... We should normally check, so as to make sure that the NN is created correctly..\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_activations)):\n",
    "            self.layers.append(Activated_Linear_Layer(num_of_inputs =layer_sizes[i],\n",
    "                                              num_of_outputs = layer_sizes[i+1],\n",
    "                                              activation = layer_activations[i],\n",
    "                                              bias = bias,\n",
    "                                              lamda = lamda,\n",
    "                                              beta = beta))\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.lamda = lamda\n",
    "    \n",
    "    #Forward run through the whole NN (predict y_hat)\n",
    "    def run (self, input_array, dropout = False):\n",
    "        input_array = np.array(input_array).T\n",
    "\n",
    "        layer_activated_output = input_array\n",
    "        for i in range(len(self.layers)):\n",
    "            layer_activated_output = self.layers[i].forward_pass(layer_activated_output)\n",
    "            #dont apply dropout if in testing accuracy, or when in final layer (i.e. softmax layer)\n",
    "            if dropout and (i < len(self.layers) - 1):\n",
    "                mask = (np.random.random_sample(layer_activated_output.shape) > self.dropout_prob) /(1 - self.dropout_prob)\n",
    "                layer_activated_output *= mask\n",
    "        return layer_activated_output\n",
    "    \n",
    "    #Train the NN on a specific input and target labels\n",
    "    def train (self, input_array, target_array, size_of_train_sample = 100, learning_rate = 0.01):\n",
    "        in_array = np.atleast_2d(np.array(input_array))\n",
    "        tar_array = np.atleast_2d(np.array(target_array))\n",
    "        \n",
    "        #calculate the predicted output of the NN\n",
    "        pred_output = self.run(in_array.T, dropout = True)  \n",
    "        #Calculate the error of the predicted output\n",
    "        #l2_cost = (self.lamda/(2*input_array.shape[0]))*(np.sum(np.square(self.hidden_layer.weights))+np.sum(np.square(self.output_layer.weights)))     \n",
    "        #we don't apply L2 here because we don't manually calculate cost and delta_cost, but skip straight to dZ = y - y_hat\n",
    "        delta_output =  pred_output - tar_array #+ l2_cost\n",
    "        mse_loss = np.average(np.square(delta_output))\n",
    "\n",
    "        #Back propogate through the NN and update weights\n",
    "        delta_activated_output = delta_output\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            delta_activated_output = self.layers[i].backward_pass(delta_activated_output, learning_rate)\n",
    "\n",
    "        return mse_loss\n",
    "\n",
    "    #Print the values of the weights for each layer    \n",
    "    def print_weights(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print('layer ', i, 'weights: \\n', self.layers[i].weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
